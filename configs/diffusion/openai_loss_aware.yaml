defaults:
  - openai_base

# Loss-aware sampling (experimental, NON-FUNCTIONAL in v1.0)
# Adaptively samples harder timesteps for better learning
# NOTE: Requires torch.distributed (DDP), not compatible with DataParallel
#       This feature is disabled in v1.0 and will only produce a warning
#       To use: migrate from DataParallel to DistributedDataParallel
timesteps: 1000
noise_schedule: cosine
sampling_mode: ddpm
use_loss_aware_sampling: true  # Currently ignored, will warn at runtime

