# AdamW with lower learning rate (5e-5)
# Use for: Larger models that are unstable at 1e-4
# Half the standard 1e-4 rate
#
# Usage: optimizer=adamw_5e5lr_wd00

optimizer_class: adamw
learning_rate: 5.0e-5
betas: [0.9, 0.999]
eps: 1.0e-8
weight_decay: 0.0
amsgrad: false

