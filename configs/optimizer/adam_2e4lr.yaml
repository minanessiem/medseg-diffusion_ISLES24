# Legacy Adam (for comparison with old runs)
# NOTE: AdamW recommended for new experiments
# Use with: python main.py optimizer=adam_2e4lr

optimizer_class: adam
learning_rate: 2.0e-4
betas: [0.9, 0.999]
eps: 1.0e-8
weight_decay: 0.0
amsgrad: false

